---
title: "Streaming Analytics into BigQuery: Challenge Lab - ARC106"
seoTitle: "Streaming Analytics into BigQuery: Challenge Lab - ARC106"
seoDescription: "In a challenge lab you’re given a scenario and a set of tasks. Instead of following step-by-step instructions, you will use the skills learned from the labs"
datePublished: Tue Apr 15 2025 04:58:21 GMT+0000 (Coordinated Universal Time)
cuid: cm9i18531002o09jxcs69cdka
slug: streaming-analytics-into-bigquery-challenge-lab-arc106
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1744692840856/31b9585e-a963-4739-9f2c-1c1ae2ff2d83.png
ogImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1744693086555/4601c947-8d7e-4025-a9c1-872c851ca0e1.png
tags: streaming-analytics-into-bigquery-challenge-lab-arc106, streaming-analytics-into-bigquery-challenge-lab, arc106

---

## Overview

In a challenge lab you’re given a scenario and a set of tasks. Instead of following step-by-step instructions, you will use the skills learned from the labs in the course to figure out how to complete the tasks on your own! An automated scoring system (shown on this page) will provide feedback on whether you have completed your tasks correctly.

When you take a challenge lab, you will not be taught new Google Cloud concepts. You are expected to extend your learned skills, like changing default values and reading and researching error messages to fix your own mistakes.

To score 100% you must successfully complete all tasks within the time period!

## Setup

### Before you click the Start Lab button

Read these instructions. Labs are timed and you cannot pause them. The timer, which starts when you click **Start Lab**, shows how long Google Cloud resources will be made available to you.

This hands-on lab lets you do the lab activities yourself in a real cloud environment, not in a simulation or demo environment. It does so by giving you new, temporary credentials that you use to sign in and access Google Cloud for the duration of the lab.

To complete this lab, you need:

* Access to a standard internet browser (Chrome browser recommended).
    

**Note:** Use an Incognito or private browser window to run this lab. This prevents any conflicts between your personal account and the Student account, which may cause extra charges incurred to your personal account.

* Time to complete the lab---remember, once you start, you cannot pause a lab.
    

**Note:** If you already have your own personal Google Cloud account or project, do not use it for this lab to avoid extra charges to your account.

## Challenge scenario

You are just starting your junior data engineer role. So far you have been helping teams create and manage data using BigQuery, Pub/Sub, and Dataflow.

You are expected to have the skills and knowledge for these tasks.

### Your challenge

You are asked to help a newly formed development team with some of their initial work on a new project around real-time environmental sensor data. You have been asked to assist the team with streaming temperature data into BigQuery using Pub/Sub and Dataflow; you receive the following request to complete the following tasks:

* Create a Cloud Storage bucket as the temporary location for a Dataflow job.
    
* Create a BigQuery dataset and table to receive the streaming data.
    
* Create up a Pub/Sub topic and test publishing messages to the topic.
    
* Create and run a Dataflow job to stream data from a Pub/Sub topic to BigQuery.
    
* Run a query to validate streaming data.
    

Some standards you should follow:

* Ensure that any needed APIs (such as Dataflow) are successfully enabled.
    
* Create all resources in the `us-central1` region, unless otherwise directed.
    

Each task is described in detail below, good luck!

## Task 1. Create a Cloud Storage bucket

* Create a Cloud Storage bucket using your Project ID as the bucket name: `qwiklabs-gcp-03-42a5b186b47b`
    

Click *Check my progress* to verify the objective.

Create a Cloud Storage bucket

**Check my progress**

## Task 2. Create a BigQuery dataset and table

1. Create a BigQuery dataset called `sensors_164` in the region named **US (multi region)**.
    
2. In the created dataset, create a table called `temperature_646` and **add** column `data` with `STRING` type.
    

Click *Check my progress* to verify the objective.

Create a BigQuery dataset and table

**Check my progress**

## Task 3. Set up a Pub/Sub topic

1. Create a Pub/Sub topic called `sensors-temp-65367`.
    

* Use the default settings, which has enabled the checkbox for **Add a default subscription**.
    

Click *Check my progress* to verify the objective.

Create a Pub/Sub topic

**Check my progress**

## Task 4. Run a Dataflow pipeline to stream data from Pub/Sub to BigQuery

1. Create and run a Dataflow job called `dfjob-46321` to stream data from Pub/Sub topic to BigQuery, using the Pub/Sub topic and BigQuery table you created in the previous tasks.
    

* Use the **Custom Dataflow Template**.
    
* Use the below Path for the template file stored in Cloud Storage:
    
    ```apache
    gs://dataflow-templates-us-central1/latest/PubSub_to_BigQuery
    ```
    
    Use the Pub/Sub topic that you created in a previous task: `sensors-temp-65367`
    
* Use the Cloud Storage bucket that you created in a previous task as the temporary location: `qwiklabs-gcp-03-42a5b186b47b`
    
* Use the BigQuery dataset and table that you created in a previous task as the output table: `sensors_164`.`temperature_646`
    
* Use `us-central1` as the regional endpoint.
    

Click **Check my progress** to verify the objective.

Create a Dataflow pipeline to stream data from Pub/Sub to BigQuery

**Check my progress**

## Task 5. Publish a test message to the topic and validate data in BigQuery

1. Publish a message to your topic using the following code syntax for **Message**: `{"data": "73.4 F"}`
    

* Note: `73.4 F` can be replaced with any value.
    

2. Run a SELECT statement in BigQuery to see the test message populated in your table.
    

**Note:** If you do not see any test messages in your BigQuery table, check that the Dataflow job has a status of *Running*, and then send another test message.

Click *Check my progress* to verify the objective.

Publish a test message to the topic and validate data in BigQuery

---

## Solution of Lab

%[https://youtu.be/9MsujO02dgM] 

```apache
export REGION=
export DATASET_NAME=
export TABLE_NAME=
export TOPIC_NAME=
export JOB_NAME=
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1744693052633/c812990b-6005-4fa8-824e-17e2233c7202.png align="center")

```apache
curl -LO raw.githubusercontent.com/Techcps/ARC/master/Streaming%20Analytics%20into%20BigQuery:%20Challenge%20Lab/techcps106.sh
sudo chmod +x techcps106.sh
./techcps106.sh
```