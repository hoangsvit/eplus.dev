---
title: "Automate Data Capture at Scale with Document AI: Challenge Lab - GSP367"
seoTitle: "Automate Data Capture at Scale with Document AI: Challenge Lab - GSP36"
seoDescription: "Learn to automate document processing with Document AI. Deploy a Google Cloud pipeline using Cloud Run, BigQuery, and Cloud Storage"
datePublished: Wed Oct 01 2025 07:47:15 GMT+0000 (Coordinated Universal Time)
cuid: cmg7oob2x000902jr652s7m76
slug: automate-data-capture-at-scale-with-document-ai-challenge-lab-gsp367
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1759304793464/a0179441-b708-4c5c-85cd-22da0919a157.png
ogImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1759304817692/cdfa49cc-f327-4d97-85e8-97c271e304b8.png
tags: automate-data-capture-at-scale-with-document-ai-challenge-lab-gsp367, automate-data-capture-at-scale-with-document-ai-challenge-lab, gsp367

---

## Overview

In a challenge lab youâ€™re given a scenario and a set of tasks. Instead of following step-by-step instructions, you will use the skills learned from the labs in the course to figure out how to complete the tasks on your own! An automated scoring system (shown on this page) will provide feedback on whether you have completed your tasks correctly.

When you take a challenge lab, you will not be taught new Google Cloud concepts. You are expected to extend your learned skills, like changing default values and reading and researching error messages to fix your own mistakes.

To score 100% you must successfully complete all tasks within the time period!

This lab is recommended for students enrolled in the [Automate Data Capture at Scale with Document AI](https://www.cloudskillsboost.google/course_templates/674) skill badge course. Are you ready for the challenge?

## Challenge scenario

You are a data engineer at large infrastructure management company and have been assigned to work on a internal project with the financial division of the company. The company has to process an ever increasing mountain of documents that all require individual manual processing for validation and authorization, which is an expensive task that requires a lot of staff. The company plans to leverage Google Cloud tools to automate the process of collecting, categorizing, and verifying documents in an efficient and less labor intensive manner.

### Your challenge

You must create a document processing pipeline that will automatically process documents that are uploaded to Cloud Storage. The pipeline consists of a primary Cloud Run functions that processes new files using a Document AI form processor to extract the data from the document. The function then saves the form data detected in those files to BigQuery.

You are provided with the source code for a Cloud Run functions that will perform the processing, and you are expected to deploy the document processing pipeline as shown in the architecture below, making sure to correctly configure the components for your specific pipeline.

![Document AI challenge lab Solution Architecture](https://cdn.qwiklabs.com/lCwLXkGYY7AC0GIZZnJ%2FSS02wt0l1owDD060Bq6EArI%3D align="left")

## Task 1. Enable the Cloud Document AI API and copy lab source files.

In this task, you enable the Cloud Document AI API and copy your starter files into Cloud Shell.

### Enable the Cloud Document AI API

* Enable the Cloud Document AI API.
    

Enable Cloud Document AI API

### Copy the lab source files into your Cloud Shell

The Cloud Run functions with predefined code is hosted on a remote Cloud Storage bucket. Copy these source files into your Cloud Shell. These files include the source code for the Cloud Run functions and the schema for the BigQuery table that you will create in the lab.

* In Cloud Shell, enter the following command to clone the source repository for the lab:
    

```apache
  mkdir ./document-ai-challenge
  gsutil -m cp -r gs://spls/gsp367/* \
    ~/document-ai-challenge/
```

## Task 2. Create a form processor

Create an instance of the general form processor using the Document AI **Form Parser** processor in the **General** (non-specialized) section. The general form processor will process any type of document and extract all the text content it can identify in the document as well as form information that it infers from the layout.

* Create the processor using the following configuration details:
    

| **Property** | **Value** |
| --- | --- |
| Processor Type | Form Parser |
| Processor Name | `form-processor` |
| Region | US |

**Note:** You will configure a Cloud Run functions later in this lab with the **PROCESSOR ID** and **PARSER LOCATION** of this processor so that the Cloud Run functions will use this specific processor to process invoices. Click on the created processor and note the PROCESSOR ID. However, the processor region is the PARSER LOCATION.

Create a form processor

## Task 3. Create Google Cloud resources

Prepare your environment by creating the Google Cloud Storage and BigQuery resources that are required for your document processing pipeline.

### Create input, output, and archive Cloud Storage buckets

* In this step, you must create the three Cloud Storage buckets listed below with uniform bucket level access enabled.
    

| **Bucket Name** | **Purpose** | **Storage class** | **Location** |
| --- | --- | --- | --- |
| `qwiklabs-gcp-04-61049cf6cac2-input-invoices` | For input invoices | Standard | `us-east4` |
| `qwiklabs-gcp-04-61049cf6cac2-output-invoices` | For storing processed data | Standard | `us-east4` |
| `qwiklabs-gcp-04-61049cf6cac2-archived-invoices` | For archiving invoices | Standard | `us-east4` |

**Note:** A bucket can be created using the gsutil tool with the *\-mb* parameter, along with *\-c* parameter to set the storage class, *\-l* to set the (regional) location and the *\-b* flag with the value of on or off to set uniform bucket level access. Read the [mb - Make buckets reference](https://cloud.google.com/storage/docs/gsutil/commands/mb) for more about creating buckets using gsutil.

### Create a BigQuery dataset and tables

* In this step, you must create a BigQuery dataset and the output table required for your data processing pipeline.
    

#### Dataset

| **Dataset Name** | **Location** |
| --- | --- |
| invoice\_parser\_results | US |

**Note:** Use **bq mk** to create BigQuery resources. The command line switch parameter *\-d* is used to create a dataset and *\--location* is used to set the location of the resource. Read the [Create datasets guide](https://cloud.google.com/bigquery/docs/datasets#create-dataset) for more information about creating datasets using the bq command-line tool.

#### Table

The table schema for the extracted information has been provided for you in the JSON file `document-ai-challenge/scripts/table-schema/doc_ai_extracted_entities.json`. Use this schema to create a table named **doc\_ai\_extracted\_entities** in the **invoice\_parser\_results** dataset.

**Note:** Use **bq mk** to create BigQuery resources. The command line switch *\--table* is used to create a table. For more information about creating tables with a schema definition using the bq command-line tool, read the [Create and use tables guide](https://cloud.google.com/bigquery/docs/tables#creating_an_empty_table_with_a_schema_definition).

You can navigate to BigQuery in the Cloud Console and inspect the schema of tables in the **invoice\_parser\_results** dataset using BigQuery SQL workspace.

Create Google Cloud resources

## Task 4. Deploy the document processing Cloud Run functions

To complete this task, you must deploy the Cloud Run functions that your data processing pipeline uses to process invoices uploaded to Cloud Storage. This function will use a Document AI API Generic Form processor to extract form data from the raw documents.

You can examine the source code of the Cloud Run functions using the Code Editor or any other editor of your choice. The Cloud Run functions is stored in the following folders in Cloud Shell:

* Process Invoices - `scripts/cloud-functions/process-invoices`
    

The Cloud Run functions, `process-invoices`, must be triggered when files are uploaded to the input files storage bucket you created earlier.

### Deploy the Cloud Run functions to process documents uploaded to Cloud Storage

Deploy a Cloud Run functions that uses a Document AI form processor to parse form documents that have been uploaded to a Cloud Storage bucket.

1. Navigate to `scripts` directory:
    

```apache
cd ~/document-ai-challenge/scripts
```

2. Assign the Artifact Registry Reader role to the Compute Engine service account:
    

```apache
PROJECT_ID=$(gcloud config get-value project)
PROJECT_NUMBER=$(gcloud projects list --filter="project_id:$PROJECT_ID" --format='value(project_number)')

SERVICE_ACCOUNT=$(gcloud storage service-agent --project=$PROJECT_ID)

gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member serviceAccount:$SERVICE_ACCOUNT \
  --role roles/pubsub.publisher
```

3. Deploy the Cloud Run functions:
    

```apache
  export CLOUD_FUNCTION_LOCATION=us-east4
  gcloud functions deploy process-invoices \
  --gen2 \
  --region=${CLOUD_FUNCTION_LOCATION} \
  --entry-point=process_invoice \
  --runtime=python39 \
  --service-account=${PROJECT_ID}@appspot.gserviceaccount.com \
  --source=cloud-functions/process-invoices \
  --timeout=400 \
  --env-vars-file=cloud-functions/process-invoices/.env.yaml \
  --trigger-resource=gs://${PROJECT_ID}-input-invoices \
  --trigger-event=google.storage.object.finalize\
  --service-account $PROJECT_NUMBER-compute@developer.gserviceaccount.com \
  --allow-unauthenticated
```

**Note:** If you get an permission error while deploying function wait for 2-3 minutes and re-run the commands.

If you inspect the Cloud Run Functions source code you will see that the function gets the Document AI processor details via `two runtime environment variables`.

* You will have to **reconfigure** the Cloud Run functions deployment so that the environment variables`PROCESSOR_ID` and `PARSER_LOCATION` contain the correct values for the **Form Parser** processor you deployed in a previous step.
    
* Make sure the PARSER\_LOCATION value `must be in lower case`.
    
* Make sure to also update the `PROJECT_ID` environment variable with your project ID.
    

Wait for the function to be fully redeployed.

Deploy Document Processing Cloud Run functions

## Task 5. Test and validate the end-to-end solution

For your final task you must successfully process the set of invoices that are available in the `~/document-ai-challenge/invoices` folder using your pipeline.

1. Upload these invoices to the input Cloud Storage bucket and monitor the progress of the pipeline.
    
2. Watch the events until you see a final event indicating that the function execution finished with a status of **OK**.
    

Once the pipeline has fully processed the documents, you will see that the form information that is extracted from the invoices by the Document AI processor has been written out into the BigQuery table.

**Note:** To monitor progress, click **Logs** in the Management section of the Cloud Run functions to view logs.

**Note :** You may see some errors that do not affect document processing significantly, especially timeouts, in this lab. If you not see data being reported as being written to BigQuery within 5 minutes in the logs then double check that the parameters set in the `.env.yaml` file in the previous section are correct and try again.

In particular make sure the Processor ID and location variables that you set are valid and note that the location parameter **must** be in lower case. Also note that the event list does not automatically refresh.

Validate data processed by the pipeline

---

## Solution of Lab

%[https://youtu.be/oJJsa6zDU04] 

```apache
curl -LO raw.githubusercontent.com/ePlus-DEV/storage/refs/heads/main/labs/GSP363/lab.sh
source lab.sh
```

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1759304894982/0ef3d916-49a3-4040-8a2d-1833822f1d69.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1759304905699/e33b885b-2e1c-4281-a6ae-567bf358b01f.png align="center")

**Script Alternative**

```apache
curl -LO https://raw.githubusercontent.com/Itsabhishek7py/GoogleCloudSkillsboost/refs/heads/main/Automate%20Data%20Capture%20at%20Scale%20with%20Document%20AI%20Challenge%20Lab/abhishek.sh
sudo chmod +x abhishek.sh
./abhishek.sh
```

**If you're not getting score on task 5 then run the below commands few times**

```apache
export PROJECT_ID=$(gcloud config get-value core/project)
gsutil -m cp -r gs://cloud-training/gsp367/* \
~/document-ai-challenge/invoices gs://${PROJECT_ID}-input-invoices/
```